---
title: "Lab 5"
format: html
embed-resources: true
author: Aditi Gajjar, Ashley Ibarra, Rebecca Ioffe
editor: visual
---

> **Goal:** Scrape information from <https://www.cheese.com> to obtain a dataset of characteristics about different cheeses, and gain deeper insight into your coding process. ðŸª¤

**Part 1:** Locate and examine the `robots.txt` file for this website. Summarize what you learn from it.

```         
User-agent: *
Sitemap: https://www.cheese.com/sitemap.xml
```

**Part 2:** Learn about the `html_attr()` function from `rvest`. Describe how this function works with a small example.

The `html_attr()` function from rvest gets a single attribute from an HTML element. HTML attributes provide additional information about elements, and are specified in the element's tag. `html_attr()` takes two arguments: the first is the HTML element, and the second is the name of the attribute to retrieve.

For example, we will use a blog review site page on the popular book, *Life of Pi*:

```{r}
#| label: part2

library(rvest)
library(tidyverse)

url <- "https://julias-books.com/2019/11/15/book-review-the-life-of-pi-by-yann-martel/"

page <- read_html(url)

image_urls <- page %>% 
  html_nodes("img") %>% 
  html_attr("src")

print(image_urls)

```

So in this example,

-   **`read_html()`** is used to read the HTML content from the webpage.

-   **`html_nodes("img")`** selects all the **`<img>`** elements in the HTML document.

-   **`html_attr("src")`** is then used to find the the source URL (**`src`** attribute) from each image tag selected (so where these images are coming from).

This has printed us the vector of the URLs for all images found on the page, which might be useful for downloading or analyzing the images further.

**Part 3:** (Do this alongside Part 4 below.) I used [ChatGPT](https://chat.openai.com/chat) to start the process of scraping cheese information with the following prompt:

> Write R code using the rvest package that allows me to scrape cheese information from cheese.com.

Fully document your process of checking this code. Record any observations you make about where ChatGPT is useful / not useful.

### Part 3 Discussion:

Running this chunk of code returns a data frame which only contains cheese names and urls from the first page of cheeses in the blog. While the data is successful extracted, we need to develop a solution to iterate through all of the pages on the blog in order to scrape all cheese names and urls - not just those on one page. ChatGPT was useful here in creating a well documented and intuitive series of steps that correctly scraped some html content from the webpage - the name of the cheese. It also used documentation that we were unfamiliar with such as paste0 and did not use a function. It was not able to correctly use all of the cheese webpage attributes, therefore, it only scraped the first page of information. Therefore, using correct attributes and iterating through all cheeses by creating vectors of urls and images, we created a data frame containing information from all 20 pages of cheeses.

```{r}
#| eval: false
#| label: small-example-of-getting-cheese-info

# Load required libraries
library(rvest)
library(dplyr)

# Define the URL
url <- "https://www.cheese.com/alphabetical"

# Read the HTML content from the webpage
webpage <- read_html(url)

# Extract the cheese names and URLs
cheese_data <- webpage %>%
  html_nodes(".cheese-item") %>%
  html_nodes("a") %>%
  html_attr("href") %>%
  paste0("https://cheese.com", .)

cheese_names <- webpage %>%
  html_nodes(".cheese-item h3") %>%
  html_text()

# Create a data frame to store the results
cheese_df <- data.frame(Name = cheese_names,
                        URL = cheese_data,
                        stringsAsFactors = FALSE)

# Print the data frame
print(cheese_df)
```

**Part 4:** Obtain the following information for **all** cheeses in the database:

-   Cheese name
-   URL for the cheese's webpage (e.g., <https://www.cheese.com/gouda/>)
-   Whether or not the cheese has a picture (e.g., [gouda](https://www.cheese.com/gouda/) has a picture, but [bianco](https://www.cheese.com/bianco/) does not)

To be kind to the website owners, please add a 1 second pause between page queries. (Note that you can view 100 cheeses at a time.)

```{r}
#| label: scraping-all-cheeses


#implementing a cheese information scraping function

# scraping a single page of cheese data
scrape_cheese_page <- function(page_number) {
  
  # considering all 20 pages of cheeses
  url <- paste0("https://www.cheese.com/alphabetical/?per_page=100&page=", page_number)
  
  webpage <- read_html(url)
  # 1 second pause
  Sys.sleep(1)  
  
  # define cheese items
  cheese_items <- webpage %>%
    html_nodes(".cheese-item")
  
  # define initial vectors
  names <- vector("character", length(cheese_items))
  urls <- vector("character", length(cheese_items))
  has_picture <- vector("logical", length(cheese_items))
  
  # For each cheese item:
  for (i in seq_along(cheese_items)) {
    # name
    names[i] <- cheese_items[i] %>%
      html_node("h3") %>%
      html_text(trim = TRUE)
    
    # url for the cheeses 
    urls[i] <- cheese_items[i] %>%
      html_node("a") %>%
      html_attr("href") %>%
      paste0("https://www.cheese.com", .)
    
    # get images if they have them
    img_node <- cheese_items[i] %>%
      html_node("img")
    
    # get the src for each image node if cheese contains a picture
    img_src <- if (!is.null(img_node)) html_attr(img_node, "src") else FALSE
    
    has_picture[i] <- !is.na(img_src) && img_src != "" && !startsWith(img_src, "/static/common")
  }
  
  # create a data frame for this page
  data_frame(
    Name = names,
    URL = urls,
    HasPicture = has_picture
  )
}

# create a data frame for all 20 pages
all_cheeses <- map_df(1:20, scrape_cheese_page)

# view all cheeses 
print(all_cheeses)


```

**Part 5:** When you go to a particular cheese's page (like [gouda](https://www.cheese.com/gouda/)), you'll see more detailed information about the cheese. For [**just 10**]{.underline} of the cheeses in the database, obtain the following detailed information:

-   milk information (summary_milk)
-   country of origin (summary_country)
-   family (summary_family)
-   type (summary_moisture_and_type)
-   flavor (summary_taste)

(Just 10 to avoid overtaxing the website! Continue adding a 1 second pause between page queries.)

```{r}
#| label: scraping-cheeses-info-function

#create a function to scrape all urls of specified cheese names

scrape_cheese_info_page <- function(cheese_name) {
  # find the URL of the cheese name specified
  url <- all_cheeses %>%
            filter(Name == cheese_name) %>%
            select(URL) %>%
            pull()
            
  webpage <- read_html(url)
  
  #cheese name attribute
  
  cheese_items <- webpage %>%
    html_nodes(".col-sm-6")
  
  #milk type attribute
  
  milk <- cheese_items %>%
    html_node(".summary_milk p") %>%
    html_text(trim = TRUE)
  
  #country of origin attribute
  
  country <- cheese_items %>%
    html_node(".summary_country p") %>%
    html_text(trim = TRUE)
  
  # family of cheese attribute
  
  family <- cheese_items %>%
    html_node(".summary_family p") %>%
    html_text(trim = TRUE)
  
  #type of cheese attribute
  
  type <- cheese_items %>%
    html_node(".summary_moisture_and_type p") %>%
    html_text(trim = TRUE)
  
  #flavor attribute
  
  flavor <- cheese_items %>%
    html_node(".summary_taste p") %>%
    html_text(trim = TRUE)
  
#create data frame of cheese information and extract only text
  
  cheese_info <- data.frame(cheese_name, milk, country, family, type, flavor)

  cheese_info <- data.frame(cheese_name, milk[2], country[2], family[2], type[2], flavor[2])

  names(cheese_info) <- c("Name", "milk_info", "origin_country", "family", "type", "flavor")
  
#return cheese_info data frame with detailed information
  
  return(cheese_info)
  
}

```

```{r}

#| label: scraping-10-cheeses-info


#initialize an empty data frame with the colnames of cheese attributes as specified above

cheese_data <- data.frame(Name = character(),
                           milk_info = character(),
                           origin_country = character(),
                           family = character(),
                           type = character(),
                           flavor = character(),
                           stringsAsFactors = FALSE)

# Loop through all 10 cheeses and apply the scrape_cheese_info_page function with one second sleep

cheeses_of_interest <- sample(all_cheeses$Name, 10)

for (cheese in cheeses_of_interest) {
  cheese_info <- scrape_cheese_info_page(cheese)
  cheese_data <- bind_rows(cheese_data, cheese_info)
  Sys.sleep(1) # Adding a 1 second delay
}

# View the scraped data
print(cheese_data)


```

**Part 6:** Evaluate the code that you wrote in terms of the [core principle of good function writing](function-strategies.qmd). To what extent does your implementation follow these principles? What are you learning about what is easy / challenging for you about approaching complex tasks requiring functions and loops?

Our function follows the principles of good function writing fairly well. In terms of approaching complex tasks with functions and loops, it is helpful to think about beginning with function inputs and outputs and work in parts. By writing pseduo code as a baseline to determine what you want the function to do, or what the loop should accomplish, you can segment the approach into manageable pieces. In terms of writing a good function, we added comments throughout each step in the function to eliminate some ambiguity in the function and to add conversation into the function. The function is reusable because we can apply the function in another context by changing the variable names, url inputs, and attributes. Our function is segmented to it is relatively short and it completes many tasks: it returns a data frame by web scraping information. By reading the source code of our function, it is fairly easy to see that our function returns a data frame of cheese names. The output of the function is not surprising to the reader. We do not create and save extraneous objects and we use package-oriented organization in the function to create intuitive steps. Finally, our function does not combine different types of objects and our function correctly returns the requested information. 
